The final processed data is ready to be used for any kind of analysis.
load("/Users/kexinsu/Documents/GitHub/Spring2020-Project1-ColleenSu08/output/processed_lyrics.RData")
load("/Users/kexinsu/Documents/GitHub/Spring2020-Project1-ColleenSu08/output/processed_lyrics.RData")
View(completed)
load("/Users/kexinsu/Documents/GitHub/Spring2020-Project1-ColleenSu08/output/processed_lyrics.RData")
---
title: "Stem Completion"
author: "Chengliang Tang, Arpita Shah, Yujie Wang and Tian Zheng"
output: html_notebook
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
"lyrics.csv" is a filtered corpus of 380,000+ song lyrics from from MetroLyrics. You can read more about it on [Kaggle](https://www.kaggle.com/gyani95/380000-lyrics-from-metrolyrics).
"artists.csv" provides the background information of all the artistis. These information are scraped from [LyricsFreak](https://www.lyricsfreak.com/).
In this R notebook, we process the raw textual data for our data analysis.
### Step 0 - Load all the required libraries
From the packages' descriptions:
+ `tm` is a framework for text mining applications within R;
+ `data.table` is a package for fast aggregation of large data;
+ `tidyverse` is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures;
+ `tidytext` allows text mining using 'dplyr', 'ggplot2', and other tidy tools;
+ `DT` provides an R interface to the JavaScript library DataTables.
```{r load libraries, warning=FALSE, message=FALSE}
library(tm)
library(data.table)
library(tidytext)
library(tidyverse)
library(DT)
```
### Step 1 - Load the data to be cleaned and processed
```{r}
# load lyrics data
load('/Users/kexinsu/Documents/GitHub/Spring2020-Project1-ColleenSu08/data/lyrics.RData')
```
### Step 2 - Preliminary cleaning of text
We clean the text by converting all the letters to the lower case, and removing punctuation, numbers, empty words and extra white space.
```{r text processing in tm}
# function for removimg leading and trailing whitespace from character strings
leadingWhitespace <- content_transformer(function(x) str_trim(x, side = "both"))
# remove stop words
data("stop_words")
word <- c("lot", "today", "months", "month", "wanna", "wouldnt", "wasnt", "ha", "na", "ooh", "da",
"gonna", "im", "dont", "aint", "wont", "yeah", "la", "oi", "nigga", "fuck",
"hey", "year", "years", "last", "past", "feel")
stop_words <- c(stop_words$word, word)
# clean the data and make a corpus
corpus <- VCorpus(VectorSource(dt_lyrics$lyrics))%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeWords, character(0))%>%
tm_map(removeWords, stop_words)%>%
tm_map(removeNumbers)%>%
tm_map(stripWhitespace)%>%
tm_map(leadingWhitespace)
```
### Step 3 - Stemming words and converting tm object to tidy object
Stemming reduces a word to its word *stem*. We stem the words here and then convert the "tm" object to a "tidy" object for much faster processing.
```{r stemming}
stemmed <- tm_map(corpus, stemDocument) %>%
tidy() %>%
select(text)
```
### Step 4 - Creating tidy format of the dictionary to be used for completing stems
We also need a dictionary to look up the words corresponding to the stems.
```{r tidy dictionary}
dict <- tidy(corpus) %>%
select(text) %>%
unnest_tokens(dictionary, text)
```
### Step 5 - Combining stems and dictionary into the same tibble
Here we combine the stems and the dictionary into the same "tidy" object.
```{r tidy stems with dictionary}
completed <- stemmed %>%
mutate(id = row_number()) %>%
unnest_tokens(stems, text) %>%
bind_cols(dict)
```
### Step 6 - Stem completion
Lastly, we complete the stems by picking the corresponding word with the highest frequency.
```{r stem completion, warning=FALSE, message=FALSE}
completed <- completed %>%
group_by(stems) %>%
count(dictionary) %>%
mutate(word = dictionary[which.max(n)]) %>%
ungroup() %>%
select(stems, word) %>%
distinct() %>%
right_join(completed) %>%
select(-stems)
```
### Step 8 - Pasting stem completed individual words into their respective lyrics
We want our processed words to resemble the structure of the original lyrics. So we paste the words together to form processed lyrics.
```{r reverse unnest}
completed <- completed %>%
group_by(id) %>%
summarise(stemmedwords= str_c(word, collapse = " ")) %>%
ungroup()
```
### Step 9 - Keeping a track of the processed lyrics with their own ID
```{r cleaned hm_data, warning=FALSE, message=FALSE}
dt_lyrics <- dt_lyrics %>%
mutate(id = row_number()) %>%
inner_join(completed)
```
### Exporting the processed text data into a CSV file
```{r export data}
save(dt_lyrics, file="/Users/kexinsu/Documents/GitHub/Spring2020-Project1-ColleenSu08/output/processed_lyrics.RData")
```
The final processed data is ready to be used for any kind of analysis.
load("/Users/kexinsu/Documents/GitHub/Spring2020-Project1-ColleenSu08/output/processed_lyrics.RData")
library(tidyverse)
library(tidytext)
library(plotly)
library(DT)
library(tm)
library(data.table)
library(scales)
library(wordcloud2)
library(gridExtra)
library(ngram)
library(shiny)
load('../data/lyrics.RData')
# load artist information
dt_artist <- fread('../data/lyrics.RData')
```{r}
lyrics_list <- c("Folk", "R&B", "Electronic", "Jazz", "Indie", "Country", "Rock", "Metal", "Pop", "Hip-Hop", "Other")
time_list <- c("1970s", "1980s", "1990s", "2000s", "2010s")
corpus <- VCorpus(VectorSource(dt_lyrics$stemmedwords))
word_tibble <- tidy(corpus) %>%
select(text) %>%
mutate(id = row_number()) %>%
unnest_tokens(word, text)
load("/Users/kexinsu/Documents/GitHub/Spring2020-Project1-ColleenSu08/output/processed_lyrics.RData")
load("/Users/kexinsu/Documents/GitHub/Spring2020-Project1-ColleenSu08/output/processed_lyrics.RData")
View(dt_lyrics)
load("/Users/kexinsu/Documents/GitHub/Spring2020-Project1-ColleenSu08/data/lyrics.RData")
View(dt_lyrics)
bias_10 <- (apply(mat.pred.1,1,mean)-true.f(10))^2
---
title: "Lab 1: Bias-Var Tradeoff & Poly Regression"
author: "Gabriel"
date: "1/26/2020"
output: pdf_document
---
Please submit your finished lab on Canvas as a knitted pdf or html document. The lab is due one week after your scheduled breakout session.
# Section I: Goal
The learning objective of this lab is to investigate the important bias-variance tradeoff in a linear regression setting under squared loss. This will be accomplished by running a small simulation study. The required tasks are stated in Section V.
# Section II: Bias-Variance Tradeoff
Let $(x_1,y_1),\ldots,(x_n,y_n)$ be the training data and denote the trained model by $\hat{f}(x)$. Consider a single test case $(x_0,y_0)$, which was not used to train $\hat{f}(x)$.  The mean square error at test case $(x_0,y_0)$ can be decomposed into three parts; i) variance of $\hat{f}(x_0)$, ii) squared bias of $\hat{f}(x_0)$, and iii) irreducible error variance $\text{Var}(\epsilon_0)$. The decomposition is stated below:
\begin{align*}
MSE(x_0)&=MSE\hat{f}(x_0)\\
&=E[(y_0-\hat{f}(x_0))^2]\\
&=\text{Var}(\hat{f}(x_0))+(E\hat{f}(x_0)-f(x_0))^2+\text{Var}(\epsilon_0)
\end{align*}
# Section III: True Model and Simulated Data
Assume $X$ is not random taking on values in the interval $[4,20]$ and the true relationship between fixed $X$ and response $Y$ is:
\[
y=f(x)+\epsilon =(x-5)(x-12)+\epsilon,
\]
where $\epsilon$ is normally distributed with $E(\epsilon)=0$ and $\text{Var}(\epsilon_0)=20^2$. Note that for test case $(x_0,y_0)$, the statistical relation between $x_0$ and $y_0$ is
\[
y_0=f(x_0)+\epsilon.
\]
The function **true.f()** defined in the code chunk below defines $f(x)$ and is evaluated at $x=16$.
```{r true.f}
true.f <- function(x) {
f.out <- (x-5)*(x-12)
return(f.out)
}
true.f(16)
```
The function **sim.training()** simulates a training dataset of size $n=20$ based on our regression model. The training feature $x$ is hard-coded and takes on equally spaced values over the interval $[4,20]$. The only input of **sim.training()** is the feature test case $x_0$.  The function returns the simulated dataset and the response for test case(s) $x_0$.
```{r data}
sim.training <- function(x.test=c(16)
) {
# Hard-coded sample size and standard deviation
n <- 20
sd.error <- 20
# Training x vector
x <- seq(4,20,length=n)
# Simulate training Y based on f(x) and normal error
y <- true.f(x)+rnorm(n,sd=sd.error)
# Simulate test case
y.test <- true.f(x.test)+rnorm(length(x.test),sd=sd.error)
# Return a list of two entries:
# 1) the dataframe data.train
# 2) test respone vector y_0
return(list(data.train=data.frame(x=x,y=y),
y.test=y.test))
}
```
To illustrate **sim.training()**, we simulate a  training dataset using **set.seed(1)**.  Two test cases are chosen at $x_0=10,16$. The following code chunk also plots the simulated data with the true relationship $f(x)$ and chosen test cases.  Make sure to run the entire code chunk at once.
```{r sim.data}
# Simulate data
set.seed(1)
x.test <- c(10,16)
sim.data.train <- sim.training(x.test=x.test)
head(sim.data.train$data.train,4)
sim.data.train$y.test
# Plot simulated data
x.plot <- seq(4,20,by=.01)
x <- sim.data.train$data.train$x
y <- sim.data.train$data.train$y
plot(x,y,main="Simulated Data")
# Plot f(x)
lines(x.plot,true.f(x.plot),lwd=2,col="red")
# Plot test cases
y.test <- sim.data.train$y.test
abline(v=x.test,col="grey")
points(x.test,y.test,pch=20,cex=1.5,col="blue")
# Legend
legend("topleft",legend=c("f(x)","Test cases"),fill=c("red","blue"),cex=.5)
```
# Section IV: Polynomial Regression and Tuning Parameter
Recall form lecture, the number of features ($p$) in a multiple linear regression model is a tuning parameter.  This naturally extends to polynomial regression as stated in the below model:
\[
y=\beta_0+\beta_1x+\beta_2x^2+\cdots+\beta_px^p+\epsilon
\]
In this case, the tuning parameter is the degree of our polynomial $p$.  High degree polynomials can approximate any continuous differentiable function. However, too high of a degree can lead to overfitting and poor generalization.
To fit this model, use the the **lm()** function in conjunction with **poly()**.  Technically we will utilize orthogonal polynomials which is not exactly the same as the polynomial regression model above. The function **predict.test.case()** defined in the code chunk below estimates $y_0$ based on the $p^{th}$ degree trained polynomial, i.e., $\hat{f}_p(x_0)$. The inputs are; i) the degree of the polynomial **degree**, ii) the training data **data**, and iii) a vector test cases **x.test**.
```{r}
predict.test.case <- function(degree,
data,
x.test) {
# Train model
model <- lm(y~poly(x,degree=degree),data=data)
# Predict test cases
y.test.hat <- predict(model,newdata=data.frame(x=x.test))
# Return y test case
return(y.test.hat)
}
```
To illustrate **predict.test.case()**, consider estimating $y_0$ for inputs $x_0=10,16$, where the polynomial is trained using our simulated data **sim.data.train**. Notice that **predict.test.case()** can also be used for plotting.
```{r}
x.plot <- seq(4,20,by=.01)
x.test <- c(10,16)
# Predict for degree=1
y.pred.1 <- predict.test.case(degree=1,
data=sim.data.train$data.train,
x.test=x.test)
y.plot.1 <- predict.test.case(degree=1,
data=sim.data.train$data.train,
x.test=x.plot)
# Predict for degree=2
y.pred.2 <- predict.test.case(degree=2,
data=sim.data.train$data.train,
x.test=x.test)
y.plot.2 <- predict.test.case(degree=2,
data=sim.data.train$data.train,
x.test=x.plot)
# Predict for degree=3
y.pred.3 <- predict.test.case(degree=3,
data=sim.data.train$data.train,
x.test=x.test)
y.plot.3 <- predict.test.case(degree=3,
data=sim.data.train$data.train,
x.test=x.plot)
# Plot simulated data
x <- sim.data.train$data.train$x
y <- sim.data.train$data.train$y
plot(x,y,main="Simulated Data")
abline(h=0,lty=2)
# Plot f(x)
lines(x.plot,true.f(x.plot),lwd=1.5,col="red")
# Plot the estimated curves
lines(x.plot,y.plot.1,lwd=1.5,col="green")
lines(x.plot,y.plot.2,lwd=1.5,col="purple")
lines(x.plot,y.plot.3,lwd=1.5,col="orange")
# Plot test cases
y.test <- sim.data.train$y.test
abline(v=x.test,col="grey")
points(x.test,y.test,pch=20,cex=1.5,col="blue")
# Plot estimated test cases
points(x.test,y.pred.1,pch=20,cex=1.5,col="green")
points(x.test,y.pred.2,pch=20,cex=1.5,col="purple")
points(x.test,y.pred.3,pch=20,cex=1.5,col="orange")
# Legend
legend("topleft",
legend=c("f(x)","Test Case","Degree 1","Degree 2","Degree 3"),
fill=c("red","blue","green","purple","orange"),
cex=.5)
```
The above code is clunky and can easily be refined. To clean up this process, the function **poly.predict()** defined below trains several polynomial models based on a vector of degrees and outputs the predicted response simultaneously. This function vectorizes **predict.test.case()**. The inputs are; i) a vector of degrees **degree.vec**, ii) a vector of $x$ test points **x.test**, iii) a training dataset **data**. The output of **poly.predict()** is a matrix where the row corresponds to the respective test cases. To see this function in action, the below code also evaluates $\hat{f}_p(x_0)$ at inputs $x_0=10,16$ using polynomial degrees 1,2,3,4.
```{r}
poly.predict <- function(degree.vec,
data,
x.test) {
# Vectorize predict.test.case()
pred <- sapply(degree.vec,
predict.test.case,
data=data,
x.test=x.test)
# Name rows and columns
rownames(pred)  <- paste("TestCase",1:length(x.test),sep="")
colnames(pred)  <- paste("D",degree.vec,sep="")
# Return
return(pred)
}
# Test function poly.predict()
x.test <- c(10,16)
poly.predict(degree.vec=1:4,
data=sim.data.train$data.train,
x.test=x.test)
```
# Section V: Student Tasks: (1) - (3)
Students will solve three major tasks in this lab.  The first task is described below.
## Task 1
Simulate **R=1000** training datasets and for each iteration (or each simulated dataset), store the predicted test cases corresponding to inputs $x_0=10,16$.  For each simulated dataset, you must predict $y_0$ using polynomial regression having degrees $p=1,2,3,4,5$. You can easily solve this problem using a loop and calling on the two functions **sim.training()** and **poly.predict()**.
Your final result will be three matrices. The first matrix **mat.pred.1** is the collection of predicted test cases for each degree corresponding to input $x_0=10$.  Similarly, the matrix **mat.pred.2** corresponds to $x_0=16$. The first two matrices are dimension $(5 \times 1000)$.  The third matrix **y.test.mat** is the collection of all test cases $y_0$ for each simulated dataset. This matrix is dimension $(2 \times 1000)$. After completing this problem, display the first three columns of each matrix.
```{r}
r=1000
mat.pred.1 <- matrix(NA,5,r)
mat.pred.2 <- matrix(NA,5,r)
y.test.mat <- matrix(NA,2,r)
x.test <- c(10,16)
for (i in 1:r){
train <- sim.training(x.test)
y.test.mat[,i] <- train$y.test
pred <- poly.predict(1:5,train$data.train,x.test)
mat.pred.1[,i] <- t(pred[1,])
mat.pred.2[,i] <- t(pred[2,])
}
mat.pred.1[,1:3]
mat.pred.2[,1:3]
y.test.mat[,1:3]
```
## Task 2
The second task is to estimate three different quantities based on the simulation from Task (1). For each polynomial degree ($p=1,2,3,4,5$), use the matrices **mat.pred.1**, **mat.pred.2** and **y.test.mat** to estimate:
\begin{enumerate}
\item The mean square error $MSE(x_0)$
\item The variance $\text{Var}(\hat{f}(x_0))$
\item The squared bias $(E\hat{f}(x_0)-f(x_0))^2$
\end{enumerate}
After solving this problem, display the 6 vectors of interest.
**Notes**:
1) When estimating the squared bias, students will use **y.test.mat** to estimate $E\hat{f}(x_0)$ but will also call the function  **true.f()**  for computing $f(x_0)$. Obviously in practice we never know the true relation $f(x)$.
2) To estimate $MSE(x_0)$, you can slightly modify your loop from Task (1) or write a new program that computes $(y_0-\hat{f}(x_0))^2$ for all $R=1000$ iterations. Then take the average of the resulting vectors.
```{r}
#1
y.test.matnew1 <- matrix(NA,5,1000)
for(i in 1:5){
y.test.matnew1[i,] <- y.test.mat[1,]
}
#SE_10 <- (mat.pred.1-y.test.matnew)^2
MSE_10 <- apply((y.test.matnew1-mat.pred.1)^2,1,mean)
y.test.matnew2 <- matrix(NA,5,1000)
for(i in 1:5){
y.test.matnew2[i,] <- y.test.mat[2,]
}
MSE_16 <- apply((y.test.matnew2-mat.pred.2)^2,1,mean)
MSE_10
MSE_16
```
```{r}
#2
var_10 <- apply(mat.pred.1,1,var)
var_16 <- apply(mat.pred.2,1,var)
var_10
var_16
```
```{r}
#3
bias_10 <- (apply(mat.pred.1,1,mean)-true.f(10))^2
bias_16 <- (apply(mat.pred.2,1,mean)-true.f(16))^2
bias_10
bias_16
```
## Task 3
The third task requires students to construct a plots showing $MSE(x_0)$, $\text{Var}(\hat{f}(x_0))$ and $\text{Bias}^2(\hat{f}(x_0))$ as a function of the polynomial degree. There should be two graphics corresponding to the two test cases $x_0=10$ and $x_0=16$.
```{r}
plot(1:5,seq(1,1000,length.out=5))
lines(1:5,MSE_10,lty=1,lwd=1.5,col="green")
lines(1:5,MSE_16,lty=2,lwd=1.5,col="green")
lines(1:5,var_10,lty=1,lwd=1.5,col="blue")
lines(1:5,var_16,lty=2,lwd=1.5,col="blue")
```
r=1000
mat.pred.1 <- matrix(NA,5,r)
mat.pred.2 <- matrix(NA,5,r)
y.test.mat <- matrix(NA,2,r)
x.test <- c(10,16)
for (i in 1:r){
train <- sim.training(x.test)
y.test.mat[,i] <- train$y.test
pred <- poly.predict(1:5,train$data.train,x.test)
mat.pred.1[,i] <- t(pred[1,])
mat.pred.2[,i] <- t(pred[2,])
}
mat.pred.1[,1:3]
mat.pred.2[,1:3]
y.test.mat[,1:3]
y.test.matnew1 <- matrix(NA,5,1000)
for(i in 1:5){
y.test.matnew1[i,] <- y.test.mat[1,]
}
#SE_10 <- (mat.pred.1-y.test.matnew)^2
MSE_10 <- apply((y.test.matnew1-mat.pred.1)^2,1,mean)
y.test.matnew2 <- matrix(NA,5,1000)
for(i in 1:5){
y.test.matnew2[i,] <- y.test.mat[2,]
}
MSE_16 <- apply((y.test.matnew2-mat.pred.2)^2,1,mean)
MSE_10
MSE_16
var_10 <- apply(mat.pred.1,1,var)
var_16 <- apply(mat.pred.2,1,var)
var_10
var_16
bias_10 <- (apply(mat.pred.1,1,mean)-true.f(10))^2
bias_16 <- (apply(mat.pred.2,1,mean)-true.f(16))^2
bias_10
bias_16
plot(1:5,MSE_10)
plot(1:5,MSE_10,xlim = (1:5),ylim=(0:1000))
plot(1:5,MSE_10,xlim = c(1,5),ylim=c(0,1000))
plot(1:5,MSE_10,xlim = c(1,5),ylim=c(0,1000),col="green")
plot(1:5,MSE_10,xlim = c(1,5),ylim=c(0,1000),pch=0,col="green")
points(1:5,MSE_16,pch=2,col="green")
plot(1:5,MSE_10,xlim = c(1,5),ylim=c(0,1000),pch=0,col="green")
points(1:5,MSE_16,pch=2,col="green")
plot(1:5,MSE_10,xlim = c(1,5),ylim=c(0,1000),pch=0,col="dark green")
plot(1:5,MSE_10,xlim = c(1,5),ylim=c(0,1000),pch=0,col="dark green")
points(1:5,MSE_16,pch=2,col="dark green")
plot(1:5,MSE_10,xlim = c(1,5),ylim=c(0,1000),pch=1,col="dark green")
points(1:5,MSE_16,pch=2,col="dark green")
plot(1:5,MSE_10,xlim = c(1,5),ylim=c(0,1000),pch=0,col="dark green")
lines(1:5,var_10,lty=1,lwd=1.5,col="dark green")
plot(1:5,MSE_10,xlim = c(1,5),ylim=c(0,1000),pch=0,col="dark green")
lines(1:5,MSE_10,lty=1,lwd=1.5,col="dark green")
plot(1:5,MSE_10,xlim = c(1,5),ylim=c(0,1000),pch=0,col="dark green")
lines(1:5,MSE_10,lty=1,lwd=1.5,col="dark green")
points(1:5,var_10,pch=1,col="blue")
lines(1:5,var_10,lty=2,lwd=1.5,col="blue")
plot(1:5,MSE_10,xlim = c(1,5),ylim=c(0,1000),pch=0,col="dark green")
lines(1:5,MSE_10,lty=1,lwd=1.5,col="dark green")
points(1:5,var_10,pch=1,col="blue")
lines(1:5,var_10,lty=2,lwd=1.5,col="blue")
points(1:5,bias_10,pch=2,col="blue")
lines(1:5,bias_10,lty=3,lwd=1.5,col="blue")
plot(1:5,MSE_10,xlim = c(1,5),ylim=c(0,1000),pch=0,col="dark green")
lines(1:5,MSE_10,lty=1,lwd=1.5,col="dark green")
points(1:5,var_10,pch=1,col="blue")
lines(1:5,var_10,lty=2,lwd=1.5,col="blue")
points(1:5,bias_10,pch=2,col="red")
lines(1:5,bias_10,lty=3,lwd=1.5,col="red")
plot(1:5,MSE_10,xlim = c(1,5),ylim=c(0,1000),pch=0,col="dark green")
lines(1:5,MSE_10,lwd=1.5,col="dark green")
points(1:5,var_10,pch=1,col="blue")
lines(1:5,var_10,lwd=1.5,col="blue")
points(1:5,bias_10,pch=2,col="red")
lines(1:5,bias_10,lwd=1.5,col="red")
plot(1:5,MSE_10,xlim = c(1,5),ylim=c(0,1000),pch=0,col="dark green")
lines(1:5,MSE_10,lwd=1.5,col="dark green")
points(1:5,var_10,pch=1,col="blue")
lines(1:5,var_10,lwd=1.5,col="blue")
points(1:5,bias_10,pch=2,col="red")
lines(1:5,bias_10,lwd=1.5,col="red")
plot(1:5,MSE_16,xlim = c(1,5),ylim=c(0,1000),pch=0,col="dark green")
lines(1:5,MSE_16,lwd=1.5,col="dark green")
points(1:5,var_16,pch=1,col="blue")
lines(1:5,var_16,lwd=1.5,col="blue")
points(1:5,bias_16,pch=2,col="red")
lines(1:5,bias_16,lwd=1.5,col="red")
plot(1:5,MSE_10,xlim = c(1,5),ylim=c(0,1000),col="dark green")
lines(1:5,MSE_10,lwd=1.5,col="dark green")
points(1:5,var_10,col="blue")
lines(1:5,var_10,lwd=1.5,col="blue")
points(1:5,bias_10,col="red")
lines(1:5,bias_10,lwd=1.5,col="red")
plot(1:5,MSE_16,xlim = c(1,5),ylim=c(0,1000),col="dark green")
lines(1:5,MSE_16,lwd=1.5,col="dark green")
points(1:5,var_16,col="blue")
lines(1:5,var_16,lwd=1.5,coxl="blue")
points(1:5,bias_16,col="red")
lines(1:5,bias_16,lwd=1.5,col="red")
plot(1:5,MSE_10,xlim = c(1,5),ylim=c(0,1000),col="dark green")
lines(1:5,MSE_10,lwd=1.5,col="dark green")
points(1:5,var_10,col="blue")
lines(1:5,var_10,lwd=1.5,col="blue")
points(1:5,bias_10,col="red")
lines(1:5,bias_10,lwd=1.5,col="red")
legend("topright",
legend=c("MSE_10","var_10","bias_10"), fill=c("green","blue","red"))
plot(1:5,MSE_10,xlim = c(1,5),ylim=c(0,1000),col="dark green")
lines(1:5,MSE_10,lwd=1.5,col="dark green")
points(1:5,var_10,col="blue")
lines(1:5,var_10,lwd=1.5,col="blue")
points(1:5,bias_10,col="red")
lines(1:5,bias_10,lwd=1.5,col="red")
legend("topright", legend=c("MSE_10","var_10","bias_10"), fill=c("dark green","blue","red"))
plot(1:5,MSE_16,xlim = c(1,5),ylim=c(0,1000),col="dark green")
lines(1:5,MSE_16,lwd=1.5,col="dark green")
points(1:5,var_16,col="blue")
lines(1:5,var_16,lwd=1.5,coxl="blue")
points(1:5,bias_16,col="red")
lines(1:5,bias_16,lwd=1.5,col="red")
legend("topright", legend=c("MSE_10","var_10","bias_10"), fill=c("dark green","blue","red"))
plot(1:5,MSE_16,xlim = c(1,5),ylim=c(0,1000),col="dark green")
lines(1:5,MSE_16,lwd=1.5,col="dark green")
points(1:5,var_16,col="blue")
lines(1:5,var_16,lwd=1.5,col="blue")
points(1:5,bias_16,col="red")
lines(1:5,bias_16,lwd=1.5,col="red")
legend("topright", legend=c("MSE_10","var_10","bias_10"), fill=c("dark green","blue","red"))
plot(1:5,MSE_10,xlim = c(1,5),ylim=c(0,1000),col="dark green",main='x.test = 10')
lines(1:5,MSE_10,lwd=1.5,col="dark green")
points(1:5,var_10,col="blue")
lines(1:5,var_10,lwd=1.5,col="blue")
points(1:5,bias_10,col="red")
lines(1:5,bias_10,lwd=1.5,col="red")
legend("topright", legend=c("MSE_10","var_10","bias_10"), fill=c("dark green","blue","red"))
